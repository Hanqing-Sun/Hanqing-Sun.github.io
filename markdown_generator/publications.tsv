pub_date	category	title	venue	excerpt	citation	url_slug	paper_url	project_url
2025-02-13	journal	Motion Expressions Guided Video Segmentation Via Effective Motion Information Mining	IEEE Transactions on Emerging Topics in Computational Intelligence	"Motion expressions guided video segmentation is aimed to segment objects in videos according to the given language descriptions about object motion. To accurately segment moving objects across frames, it is important to capture motion information of objects within the entire video. However, the existing method fails to encode object motion information accurately. In this paper, we propose an effective motion information mining framework to improve motion expressions guided video segmentation, named EMIM. It consists of two novel modules, including a hierarchical motion aggregation module and a box-level positional encoding module. Specifically, the hierarchical motion aggregation module is aimed to capture local and global temporal information of objects within a video. To achieve this goal, we introduce local-window self-attention and selective state space models for short-term and long-term feature aggregation. Inspired by that the spatial changes of objects can effectively reflect the object motion across frames, the box-level positional encoding module integrates object spatial information into object embeddings. With two proposed modules, our proposed method can capture object spatial changes with temporal evolution. We conduct the extensive experiments on motion expressions guided video segmentation dataset MeViS to reveal the advantages of our EMIM. Our proposed EMIM achieves a J&F score of 42.2%, outperforming the prior approach, LMPM, by 5.0%."	"Li Ge, Sun Hanqing, Yang Aiping, Cao Jiale, and Pang Yanwei. Motion Expressions Guided Video Segmentation Via Effective Motion Information Mining. <i>IEEE Transactions on Emerging Topics in Computational Intelligence</i>, 2024."	emim	https://doi.org/10.1109/TETCI.2025.3537936	
2024-12-25	journal	Video instance segmentation without using mask and identity supervision	IEEE Transactions on Multimedia	"Video instance segmentation (VIS) is a challenging vision problem in which the task is to simultaneously detect, segment, and track all the object instances in a video. Most existing VIS approaches rely on pixel-level mask supervision within a frame as well as instance-level identity annotation across frames. However, obtaining these 'mask and identity' annotations is timeconsuming and expensive. We propose the first mask-identityfree VIS framework that neither utilizes mask annotations nor requires identity supervision. Accordingly, we introduce a query contrast and exchange network (QCEN) comprising instance query contrast and query-exchanged mask learning. The instance query contrast first performs cross-frame instance matching and then conducts query feature contrastive learning. The query-exchanged mask learning exploits both intra-video and inter-video query exchange properties: exchanging queries of an identical instance from different frames within a video results in consistent instance masks, whereas exchanging queries across videos results in all-zero background masks. Extensive experiments on three benchmarks (YouTube-VIS 2019, YouTube-VIS 2021, and OVIS) reveal the merits of the proposed approach, which significantly reduces the performance gap between the identify-free baseline and our maskidentify-free VIS method. On the YouTube-VIS 2019 validation set, our mask-identity-free approach achieves 91.4% of the strongersupervision-based baseline performance when utilizing the same ImageNet pre-trained model."	"Li Ge, Cao Jiale, Sun Hanqing, Anwer Rao M., Xie Jin, Khan Fahad, Pang Yanwei. Video instance segmentation without using mask and identity supervision. <i>IEEE Transactions on Multimedia</i>, 2025, 27: 224-235."	mifvis	https://doi.org/10.1109/TMM.2024.3521668	
2024-09-30	journal	Transformer-based stereo-aware 3D object detection from binocular images	IEEE Transactions on Intelligent Transportation Systems	"Transformers have shown promising progress in various visual object detection tasks, including monocular 2D/3D detection and surround-view 3D detection. More importantly, the attention mechanism in the Transformer model and the 3D information extraction in binocular stereo are both similaritybased. However, directly applying existing Transformer-based detectors to binocular stereo 3D object detection leads to slow convergence and significant precision drops. We argue that a key cause of that defect is that existing Transformers ignore the binocular-stereo-specific image correspondence information. In this paper, we explore the model design of Transformers in binocular 3D object detection, focusing particularly on extracting and encoding task-specific image correspondence information. To achieve this goal, we present TS3D, a Transformer-based Stereo-aware 3D object detector. In the TS3D, a DisparityAware Positional Encoding (DAPE) module is proposed to embed the image correspondence information into stereo features. The correspondence is encoded as normalized sub-pixel-level disparity and is used in conjunction with sinusoidal 2D positional encoding to provide the 3D location information of the scene. To enrich multi-scale stereo features, we propose a Stereo Preserving Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the correspondence information while fusing intra-scale and aggregating cross-scale stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection average precision on the KITTI test set and takes 88 ms to detect objects from each binocular image pair. It is competitive with advanced counterparts in terms of both precision and inference speed."	"Sun Hanqing, Pang Yanwei, Cao Jiale, Xie Jin, Li Xuelong. Transformer-based stereo-aware 3D object detection from binocular images. <i>IEEE Transactions on Intelligent Transportation Systems</i>, 2024, 25(12): 19675-19687."	ts3d	https://doi.org/10.1109/TITS.2024.3462795	
2024-04-17	journal	Remote sensing image dehazing via a local context-enriched Transformer	Remote Sensing	"Remote sensing image dehazing is a well-known remote sensing image processing task focused on restoring clean images from hazy images. The Transformer network, based on the selfattention mechanism, has demonstrated remarkable advantages in various image restoration tasks, due to its capacity to capture long-range dependencies within images. However, it is weak at modeling local context. Conversely, convolutional neural networks (CNNs) are adept at capturing local contextual information. Local contextual information could provide more details, while long-range dependencies capture global structure information. The combination of long-range dependencies and local context modeling is beneficial for remote sensing image dehazing. Therefore, in this paper, we propose a CNN-based adaptive local context enrichment module (ALCEM) to extract contextual information within local regions. Subsequently, we integrate our proposed ALCEM into the multi-head self-attention and feed-forward network of the Transformer, constructing a novel locally enhanced attention (LEA) and a local continuous-enhancement feed-forward network (LCFN). The LEA utilizes the ALCEM to inject local context information that is complementary to the long-range relationship modeled by multi-head self-attention, which is beneficial to removing haze and restoring details. The LCFN extracts multi-scale spatial information and selectively fuses them by the the ALCEM, which supplements more informative information compared with existing regular feed-forward networks with only position-specific information flow. Powered by the LEA and LCFN, a novel Transformer-based dehazing network termed LCEFormer is proposed to restore clear images from hazy remote sensing images, which combines the advantages of CNN and Transformer. Experiments conducted on three distinct datasets, namely DHID, ERICE, and RSID, demonstrate that our proposed LCEFormer achieves the state-of-the-art performance in hazy scenes. Specifically, our LCEFormer outperforms DCIL by 0.78 dB and 0.018 for PSNR and SSIM on the DHID dataset."	"Nie Jing, Xie Jin, Sun Hanqing. Remote sensing image dehazing via a local context-enriched Transformer. <i>Remote Sensing</i>, 2024, 16(8): 1422."	lceformer	https://doi.org/10.3390/rs16081422	
2023-10-14	journal	Deep intra-image contrastive learning for weakly supervised one-step person search	Pattern Recognition	"Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusionlevel variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scaleinvariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches."	"Wang Jiabei, Pang Yanwei, Cao Jiale, Sun Hanqing, Shao Zhuang, Li Xuelong. Deep intra-image contrastive learning for weakly supervised one-step person search. <i>Pattern Recognition</i>, 2024, 147: 110047."	dicl	https://doi.org/10.1016/j.patcog.2023.110047	
2023-06-20	journal	Supervised biadjacency networks for stereo matching	Multimedia Tools and Applications	"Convolutional neural network (CNN) based stereo matching methods using cost volume techniques have gained prominence in stereo matching. State-of-the-art cost volume based methods use two weight-sharing feature extractors to respectively extract left and right unary features and then use them to construct cost volume(s). The quality of those unary features is crucial for the subsequent stereo matching. We propose a Supervised Biadjacency-based (SuperB) module to improve their quality by employing supervised biadjacency matrices to embed stereo information into both unary features. Specifically, disparity supervision is imposed on the biadjacency matrices by transforming them into disparity estimations. The SuperB Module can therefore adaptively enhance matched features and suppress unmatched features. Being aware of the stereo correspondence, the resultant stereo-aware features are more discriminative for subsequent cost aggregation and disparity estimation. Experiments show the SuperB Module can be plugged into cost volume based stereo matching models and lower the disparity estimation error. In addition, a scaleadaptive Voxel-wise Selective Fusion (VSF) module is proposed to adaptively aggregate the multi-scale matching costs. The competitive and efficient experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the resultant Supervised Biadjacency Stereo matching networks (SuperBStereo)."	"Sun Hanqing, Han Jungong, Pang Yanwei, Li Xuelong. Supervised biadjacency networks for stereo matching. <i>Multimedia Tools and Applications</i>, 2023, 83(4): 10247-10272."	superbstereo	https://doi.org/10.1007/s11042-023-15362-5	
2023-02-05	journal	Semantic-aware self-supervised depth estimation for stereo 3D detection	Pattern Recognition Letters	"Besides the 3D object supervision, the auxiliary disparity supervision is usually indispensable when training a stereo-based 3D object detector. The disparity supervision is either transformed from LiDAR points or generated from pre-trained models. However, the former suffers from the high cost and over-sensitivity to airborne particles of LiDAR devices, and the latter from the limited cross-dataset transferability of contemporary stereo matching models. To alleviate those problems, we propose a self-supervision framework for stereo-based 3D detection that relies on neither LiDARs nor external models. A Depth-based Self-supervision (DSelf) is proposed to unify the coordinate spaces of self-supervised losses and detection into a 3D space. However, the DSelf supervision is dense compared with the sparse LiDAR points, which introduces redundancy and irrelevancy into the stereo features. A Semantic-Aware Sampler (SASampler) is proposed to address the problems by an unbalanced sampling of foreground and background pixels. Combining our SASampler and DSelf supervision, the resultant detector (named S3D) achieves state-ofthe-art detection results without explicit disparity supervisions."	"Sun Hanqing, Cao Jiale, Pang Yanwei. Semantic-aware self-supervised depth estimation for stereo 3D detection. <i>Pattern Recognition Letters</i>, 2023, 167: 164-170."	s3d	https://doi.org/10.1016/j.patrec.2023.02.006	
2020-11-03	journal	TJU-DHD: A diverse high-resolution dataset for object detection	IEEE Transactions on Image Processing	"Vehicles, pedestrians, and riders are the most important and interesting objects for the perception modules of self-driving vehicles and video surveillance. However, the stateof-the-art performance of detecting such important objects (esp. small objects) is far from satisfying the demand of practical systems. Large-scale, rich-diversity, and high-resolution datasets play an important role in developing better object detection methods to satisfy the demand. Existing public large-scale datasets such as MS COCO collected from websites do not focus on the specific scenarios. Moreover, the popular datasets (e.g., KITTI and Citypersons) collected from the specific scenarios are limited in the number of images and instances, the resolution, and the diversity. To attempt to solve the problem, we build a diverse high-resolution dataset (called TJU-DHD). The dataset contains 115 354 high-resolution images (52% images have a resolution of 1624 x 1200 pixels and 48% images have a resolution of at least 2,560 x 1,440 pixels) and 709 330 labeled objects in total with a large variance in scale and appearance. Meanwhile, the dataset has a rich diversity in season variance, illumination variance, and weather variance. In addition, a new diverse pedestrian dataset is further built. With the four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS, two-stage FPN, and Cascade R-CNN), experiments about object detection and pedestrian detection are conducted. We hope that the newly built dataset can help promote the research on object detection and pedestrian detection in these two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD."	"Pang Yanwei, Cao Jiale, Li Yazhao, Xie Jin, Sun Hanqing, Gong Jinfeng. TJU-DHD: A diverse high-resolution dataset for object detection. <i>IEEE Transactions on Image Processing</i>, 2021, 30: 207-219."	tju-dhd	https://doi.org/10.1109/TIP.2020.3034487	https://github.com/tjubiit/TJU-DHD
2019-07-14	journal	Dual-resolution dual-path convolutional neural networks for fast object detection	Sensors	"Downsampling input images is a simple trick to speed up visual object-detection algorithms, especially on robotic vision and applied mobile vision systems. However, this trick comes with a significant decline in accuracy. In this paper, dual-resolution dual-path Convolutional Neural Networks (CNNs), named DualNets, are proposed to bump up the accuracy of those detection applications. In contrast to previous methods that simply downsample the input images, DualNets explicitly take dual inputs in different resolutions and extract complementary visual features from these using dual CNN paths. The two paths in a DualNet are a backbone path and an auxiliary path that accepts larger inputs and then rapidly downsamples them to relatively small feature maps. With the help of the carefully designed auxiliary CNN paths in DualNets, auxiliary features are extracted from the larger input with controllable computation. Auxiliary features are then fused with the backbone features using a proposed progressive residual fusion strategy to enrich feature representation.This architecture, as the feature extractor, is further integrated with the Single Shot Detector (SSD) to accomplish latency-sensitive visual object-detection tasks. We evaluate the resulting detection pipeline on Pascal VOC and MS COCO benchmarks. Results show that the proposed DualNets can raise the accuracy of those CNN detection applications that are sensitive to computation payloads."	"Pan Jing, Sun Hanqing, Song Zhanjie, Han Jungong*. Dual-resolution dual-path convolutional neural networks for fast object detection. <i>Sensors</i>, 2019, 19(14), 3111."	dualnet	https://doi.org/10.3390/s19143111	
2018-09-03	journal	GlanceNets - efficient convolutional neural networks with adaptive hard example mining	Science China Information Sciences	"Despite the success of CNNs, it is impeded to deploy such deep CNN models in real-time tasks due to high computational complexity. To address the problem, we propose GlanceNets with several bypasses (Figure 1). In modern CNNs, it is believed that shallow layers provide lower-level features, whereas deep layers correspond to higherlevel features. However, it is not always necessary to classify a sample with the highest-level feature. In many cases, easy samples can be correctly classified with low-level features, just as one can recognize common items at a glance. Such observation is the key motivation of proposed GlanceNets in this study."	"Sun Hanqing, Pang Yanwei. GlanceNets - efficient convolutional neural networks with adaptive hard example mining. <i>Science China Information Sciences</i>, 2018, 61(10): 109101."	glancenet	https://doi.org/10.1007/s11432-018-9497-0	