{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Publications markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `publications.py`. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one containing your data.\n",
    "\n",
    "TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n",
    "\n",
    "- `excerpt` and `paper_url` can be blank, but the others must have values. \n",
    "- `pub_date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub_date\tcategory\ttitle\tvenue\texcerpt\tcitation\turl_slug\tpaper_url\tproject_url\n",
      "2025-02-13\tjournal\tMotion Expressions Guided Video Segmentation Via Effective Motion Information Mining\tIEEE Transactions on Emerging Topics in Computational Intelligence\t\"Motion expressions guided video segmentation is aimed to segment objects in videos according to the given language descriptions about object motion. To accurately segment moving objects across frames, it is important to capture motion information of objects within the entire video. However, the existing method fails to encode object motion information accurately. In this paper, we propose an effective motion information mining framework to improve motion expressions guided video segmentation, named EMIM. It consists of two novel modules, including a hierarchical motion aggregation module and a box-level positional encoding module. Specifically, the hierarchical motion aggregation module is aimed to capture local and global temporal information of objects within a video. To achieve this goal, we introduce local-window self-attention and selective state space models for short-term and long-term feature aggregation. Inspired by that the spatial changes of objects can effectively reflect the object motion across frames, the box-level positional encoding module integrates object spatial information into object embeddings. With two proposed modules, our proposed method can capture object spatial changes with temporal evolution. We conduct the extensive experiments on motion expressions guided video segmentation dataset MeViS to reveal the advantages of our EMIM. Our proposed EMIM achieves a J&F score of 42.2%, outperforming the prior approach, LMPM, by 5.0%.\"\t\"Li Ge, Sun Hanqing, Yang Aiping, Cao Jiale, and Pang Yanwei. Motion Expressions Guided Video Segmentation Via Effective Motion Information Mining. <i>IEEE Transactions on Emerging Topics in Computational Intelligence</i>, 2024.\"\temim\thttps://doi.org/10.1109/TETCI.2025.3537936\t\n",
      "2024-12-25\tjournal\tVideo instance segmentation without using mask and identity supervision\tIEEE Transactions on Multimedia\t\"Video instance segmentation (VIS) is a challenging vision problem in which the task is to simultaneously detect, segment, and track all the object instances in a video. Most existing VIS approaches rely on pixel-level mask supervision within a frame as well as instance-level identity annotation across frames. However, obtaining these 'mask and identity' annotations is timeconsuming and expensive. We propose the first mask-identityfree VIS framework that neither utilizes mask annotations nor requires identity supervision. Accordingly, we introduce a query contrast and exchange network (QCEN) comprising instance query contrast and query-exchanged mask learning. The instance query contrast first performs cross-frame instance matching and then conducts query feature contrastive learning. The query-exchanged mask learning exploits both intra-video and inter-video query exchange properties: exchanging queries of an identical instance from different frames within a video results in consistent instance masks, whereas exchanging queries across videos results in all-zero background masks. Extensive experiments on three benchmarks (YouTube-VIS 2019, YouTube-VIS 2021, and OVIS) reveal the merits of the proposed approach, which significantly reduces the performance gap between the identify-free baseline and our maskidentify-free VIS method. On the YouTube-VIS 2019 validation set, our mask-identity-free approach achieves 91.4% of the strongersupervision-based baseline performance when utilizing the same ImageNet pre-trained model.\"\t\"Li Ge, Cao Jiale, Sun Hanqing, Anwer Rao M., Xie Jin, Khan Fahad, Pang Yanwei. Video instance segmentation without using mask and identity supervision. <i>IEEE Transactions on Multimedia</i>, 2025, 27: 224-235.\"\tmifvis\thttps://doi.org/10.1109/TMM.2024.3521668\t\n",
      "2024-09-30\tjournal\tTransformer-based stereo-aware 3D object detection from binocular images\tIEEE Transactions on Intelligent Transportation Systems\t\"Transformers have shown promising progress in various visual object detection tasks, including monocular 2D/3D detection and surround-view 3D detection. More importantly, the attention mechanism in the Transformer model and the 3D information extraction in binocular stereo are both similaritybased. However, directly applying existing Transformer-based detectors to binocular stereo 3D object detection leads to slow convergence and significant precision drops. We argue that a key cause of that defect is that existing Transformers ignore the binocular-stereo-specific image correspondence information. In this paper, we explore the model design of Transformers in binocular 3D object detection, focusing particularly on extracting and encoding task-specific image correspondence information. To achieve this goal, we present TS3D, a Transformer-based Stereo-aware 3D object detector. In the TS3D, a DisparityAware Positional Encoding (DAPE) module is proposed to embed the image correspondence information into stereo features. The correspondence is encoded as normalized sub-pixel-level disparity and is used in conjunction with sinusoidal 2D positional encoding to provide the 3D location information of the scene. To enrich multi-scale stereo features, we propose a Stereo Preserving Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the correspondence information while fusing intra-scale and aggregating cross-scale stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection average precision on the KITTI test set and takes 88 ms to detect objects from each binocular image pair. It is competitive with advanced counterparts in terms of both precision and inference speed.\"\t\"Sun Hanqing, Pang Yanwei, Cao Jiale, Xie Jin, Li Xuelong. Transformer-based stereo-aware 3D object detection from binocular images. <i>IEEE Transactions on Intelligent Transportation Systems</i>, 2024, 25(12): 19675-19687.\"\tts3d\thttps://doi.org/10.1109/TITS.2024.3462795\t\n",
      "2024-04-17\tjournal\tRemote sensing image dehazing via a local context-enriched Transformer\tRemote Sensing\t\"Remote sensing image dehazing is a well-known remote sensing image processing task focused on restoring clean images from hazy images. The Transformer network, based on the selfattention mechanism, has demonstrated remarkable advantages in various image restoration tasks, due to its capacity to capture long-range dependencies within images. However, it is weak at modeling local context. Conversely, convolutional neural networks (CNNs) are adept at capturing local contextual information. Local contextual information could provide more details, while long-range dependencies capture global structure information. The combination of long-range dependencies and local context modeling is beneficial for remote sensing image dehazing. Therefore, in this paper, we propose a CNN-based adaptive local context enrichment module (ALCEM) to extract contextual information within local regions. Subsequently, we integrate our proposed ALCEM into the multi-head self-attention and feed-forward network of the Transformer, constructing a novel locally enhanced attention (LEA) and a local continuous-enhancement feed-forward network (LCFN). The LEA utilizes the ALCEM to inject local context information that is complementary to the long-range relationship modeled by multi-head self-attention, which is beneficial to removing haze and restoring details. The LCFN extracts multi-scale spatial information and selectively fuses them by the the ALCEM, which supplements more informative information compared with existing regular feed-forward networks with only position-specific information flow. Powered by the LEA and LCFN, a novel Transformer-based dehazing network termed LCEFormer is proposed to restore clear images from hazy remote sensing images, which combines the advantages of CNN and Transformer. Experiments conducted on three distinct datasets, namely DHID, ERICE, and RSID, demonstrate that our proposed LCEFormer achieves the state-of-the-art performance in hazy scenes. Specifically, our LCEFormer outperforms DCIL by 0.78 dB and 0.018 for PSNR and SSIM on the DHID dataset.\"\t\"Nie Jing, Xie Jin, Sun Hanqing. Remote sensing image dehazing via a local context-enriched Transformer. <i>Remote Sensing</i>, 2024, 16(8): 1422.\"\tlceformer\thttps://doi.org/10.3390/rs16081422\t\n",
      "2023-10-14\tjournal\tDeep intra-image contrastive learning for weakly supervised one-step person search\tPattern Recognition\t\"Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusionlevel variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scaleinvariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches.\"\t\"Wang Jiabei, Pang Yanwei, Cao Jiale, Sun Hanqing, Shao Zhuang, Li Xuelong. Deep intra-image contrastive learning for weakly supervised one-step person search. <i>Pattern Recognition</i>, 2024, 147: 110047.\"\tdicl\thttps://doi.org/10.1016/j.patcog.2023.110047\t\n",
      "2023-06-20\tjournal\tSupervised biadjacency networks for stereo matching\tMultimedia Tools and Applications\t\"Convolutional neural network (CNN) based stereo matching methods using cost volume techniques have gained prominence in stereo matching. State-of-the-art cost volume based methods use two weight-sharing feature extractors to respectively extract left and right unary features and then use them to construct cost volume(s). The quality of those unary features is crucial for the subsequent stereo matching. We propose a Supervised Biadjacency-based (SuperB) module to improve their quality by employing supervised biadjacency matrices to embed stereo information into both unary features. Specifically, disparity supervision is imposed on the biadjacency matrices by transforming them into disparity estimations. The SuperB Module can therefore adaptively enhance matched features and suppress unmatched features. Being aware of the stereo correspondence, the resultant stereo-aware features are more discriminative for subsequent cost aggregation and disparity estimation. Experiments show the SuperB Module can be plugged into cost volume based stereo matching models and lower the disparity estimation error. In addition, a scaleadaptive Voxel-wise Selective Fusion (VSF) module is proposed to adaptively aggregate the multi-scale matching costs. The competitive and efficient experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the resultant Supervised Biadjacency Stereo matching networks (SuperBStereo).\"\t\"Sun Hanqing, Han Jungong, Pang Yanwei, Li Xuelong. Supervised biadjacency networks for stereo matching. <i>Multimedia Tools and Applications</i>, 2023, 83(4): 10247-10272.\"\tsuperbstereo\thttps://doi.org/10.1007/s11042-023-15362-5\t\n",
      "2023-02-05\tjournal\tSemantic-aware self-supervised depth estimation for stereo 3D detection\tPattern Recognition Letters\t\"Besides the 3D object supervision, the auxiliary disparity supervision is usually indispensable when training a stereo-based 3D object detector. The disparity supervision is either transformed from LiDAR points or generated from pre-trained models. However, the former suffers from the high cost and over-sensitivity to airborne particles of LiDAR devices, and the latter from the limited cross-dataset transferability of contemporary stereo matching models. To alleviate those problems, we propose a self-supervision framework for stereo-based 3D detection that relies on neither LiDARs nor external models. A Depth-based Self-supervision (DSelf) is proposed to unify the coordinate spaces of self-supervised losses and detection into a 3D space. However, the DSelf supervision is dense compared with the sparse LiDAR points, which introduces redundancy and irrelevancy into the stereo features. A Semantic-Aware Sampler (SASampler) is proposed to address the problems by an unbalanced sampling of foreground and background pixels. Combining our SASampler and DSelf supervision, the resultant detector (named S3D) achieves state-ofthe-art detection results without explicit disparity supervisions.\"\t\"Sun Hanqing, Cao Jiale, Pang Yanwei. Semantic-aware self-supervised depth estimation for stereo 3D detection. <i>Pattern Recognition Letters</i>, 2023, 167: 164-170.\"\ts3d\thttps://doi.org/10.1016/j.patrec.2023.02.006\t\n",
      "2020-11-03\tjournal\tTJU-DHD: A diverse high-resolution dataset for object detection\tIEEE Transactions on Image Processing\t\"Vehicles, pedestrians, and riders are the most important and interesting objects for the perception modules of self-driving vehicles and video surveillance. However, the stateof-the-art performance of detecting such important objects (esp. small objects) is far from satisfying the demand of practical systems. Large-scale, rich-diversity, and high-resolution datasets play an important role in developing better object detection methods to satisfy the demand. Existing public large-scale datasets such as MS COCO collected from websites do not focus on the specific scenarios. Moreover, the popular datasets (e.g., KITTI and Citypersons) collected from the specific scenarios are limited in the number of images and instances, the resolution, and the diversity. To attempt to solve the problem, we build a diverse high-resolution dataset (called TJU-DHD). The dataset contains 115 354 high-resolution images (52% images have a resolution of 1624 x 1200 pixels and 48% images have a resolution of at least 2,560 x 1,440 pixels) and 709 330 labeled objects in total with a large variance in scale and appearance. Meanwhile, the dataset has a rich diversity in season variance, illumination variance, and weather variance. In addition, a new diverse pedestrian dataset is further built. With the four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS, two-stage FPN, and Cascade R-CNN), experiments about object detection and pedestrian detection are conducted. We hope that the newly built dataset can help promote the research on object detection and pedestrian detection in these two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD.\"\t\"Pang Yanwei, Cao Jiale, Li Yazhao, Xie Jin, Sun Hanqing, Gong Jinfeng. TJU-DHD: A diverse high-resolution dataset for object detection. <i>IEEE Transactions on Image Processing</i>, 2021, 30: 207-219.\"\ttju-dhd\thttps://doi.org/10.1109/TIP.2020.3034487\thttps://github.com/tjubiit/TJU-DHD\n",
      "2019-07-14\tjournal\tDual-resolution dual-path convolutional neural networks for fast object detection\tSensors\t\"Downsampling input images is a simple trick to speed up visual object-detection algorithms, especially on robotic vision and applied mobile vision systems. However, this trick comes with a significant decline in accuracy. In this paper, dual-resolution dual-path Convolutional Neural Networks (CNNs), named DualNets, are proposed to bump up the accuracy of those detection applications. In contrast to previous methods that simply downsample the input images, DualNets explicitly take dual inputs in different resolutions and extract complementary visual features from these using dual CNN paths. The two paths in a DualNet are a backbone path and an auxiliary path that accepts larger inputs and then rapidly downsamples them to relatively small feature maps. With the help of the carefully designed auxiliary CNN paths in DualNets, auxiliary features are extracted from the larger input with controllable computation. Auxiliary features are then fused with the backbone features using a proposed progressive residual fusion strategy to enrich feature representation.This architecture, as the feature extractor, is further integrated with the Single Shot Detector (SSD) to accomplish latency-sensitive visual object-detection tasks. We evaluate the resulting detection pipeline on Pascal VOC and MS COCO benchmarks. Results show that the proposed DualNets can raise the accuracy of those CNN detection applications that are sensitive to computation payloads.\"\t\"Pan Jing, Sun Hanqing, Song Zhanjie, Han Jungong*. Dual-resolution dual-path convolutional neural networks for fast object detection. <i>Sensors</i>, 2019, 19(14), 3111.\"\tdualnet\thttps://doi.org/10.3390/s19143111\t\n",
      "2018-09-03\tjournal\tGlanceNets - efficient convolutional neural networks with adaptive hard example mining\tScience China Information Sciences\t\"Despite the success of CNNs, it is impeded to deploy such deep CNN models in real-time tasks due to high computational complexity. To address the problem, we propose GlanceNets with several bypasses (Figure 1). In modern CNNs, it is believed that shallow layers provide lower-level features, whereas deep layers correspond to higherlevel features. However, it is not always necessary to classify a sample with the highest-level feature. In many cases, easy samples can be correctly classified with low-level features, just as one can recognize common items at a glance. Such observation is the key motivation of proposed GlanceNets in this study.\"\t\"Sun Hanqing, Pang Yanwei. GlanceNets - efficient convolutional neural networks with adaptive hard example mining. <i>Science China Information Sciences</i>, 2018, 61(10): 109101.\"\tglancenet\thttps://doi.org/10.1007/s11432-018-9497-0\t"
     ]
    }
   ],
   "source": [
    "!cat publications.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "\n",
    "We are using the very handy pandas library for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>citation</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>project_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>journal</td>\n",
       "      <td>Motion Expressions Guided Video Segmentation V...</td>\n",
       "      <td>IEEE Transactions on Emerging Topics in Comput...</td>\n",
       "      <td>Motion expressions guided video segmentation i...</td>\n",
       "      <td>Li Ge, Sun Hanqing, Yang Aiping, Cao Jiale, an...</td>\n",
       "      <td>emim</td>\n",
       "      <td>https://doi.org/10.1109/TETCI.2025.3537936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-25</td>\n",
       "      <td>journal</td>\n",
       "      <td>Video instance segmentation without using mask...</td>\n",
       "      <td>IEEE Transactions on Multimedia</td>\n",
       "      <td>Video instance segmentation (VIS) is a challen...</td>\n",
       "      <td>Li Ge, Cao Jiale, Sun Hanqing, Anwer Rao M., X...</td>\n",
       "      <td>mifvis</td>\n",
       "      <td>https://doi.org/10.1109/TMM.2024.3521668</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>journal</td>\n",
       "      <td>Transformer-based stereo-aware 3D object detec...</td>\n",
       "      <td>IEEE Transactions on Intelligent Transportatio...</td>\n",
       "      <td>Transformers have shown promising progress in ...</td>\n",
       "      <td>Sun Hanqing, Pang Yanwei, Cao Jiale, Xie Jin, ...</td>\n",
       "      <td>ts3d</td>\n",
       "      <td>https://doi.org/10.1109/TITS.2024.3462795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>journal</td>\n",
       "      <td>Remote sensing image dehazing via a local cont...</td>\n",
       "      <td>Remote Sensing</td>\n",
       "      <td>Remote sensing image dehazing is a well-known ...</td>\n",
       "      <td>Nie Jing, Xie Jin, Sun Hanqing. Remote sensing...</td>\n",
       "      <td>lceformer</td>\n",
       "      <td>https://doi.org/10.3390/rs16081422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>journal</td>\n",
       "      <td>Deep intra-image contrastive learning for weak...</td>\n",
       "      <td>Pattern Recognition</td>\n",
       "      <td>Weakly supervised person search aims to perfor...</td>\n",
       "      <td>Wang Jiabei, Pang Yanwei, Cao Jiale, Sun Hanqi...</td>\n",
       "      <td>dicl</td>\n",
       "      <td>https://doi.org/10.1016/j.patcog.2023.110047</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>journal</td>\n",
       "      <td>Supervised biadjacency networks for stereo mat...</td>\n",
       "      <td>Multimedia Tools and Applications</td>\n",
       "      <td>Convolutional neural network (CNN) based stere...</td>\n",
       "      <td>Sun Hanqing, Han Jungong, Pang Yanwei, Li Xuel...</td>\n",
       "      <td>superbstereo</td>\n",
       "      <td>https://doi.org/10.1007/s11042-023-15362-5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-02-05</td>\n",
       "      <td>journal</td>\n",
       "      <td>Semantic-aware self-supervised depth estimatio...</td>\n",
       "      <td>Pattern Recognition Letters</td>\n",
       "      <td>Besides the 3D object supervision, the auxilia...</td>\n",
       "      <td>Sun Hanqing, Cao Jiale, Pang Yanwei. Semantic-...</td>\n",
       "      <td>s3d</td>\n",
       "      <td>https://doi.org/10.1016/j.patrec.2023.02.006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>journal</td>\n",
       "      <td>TJU-DHD: A diverse high-resolution dataset for...</td>\n",
       "      <td>IEEE Transactions on Image Processing</td>\n",
       "      <td>Vehicles, pedestrians, and riders are the most...</td>\n",
       "      <td>Pang Yanwei, Cao Jiale, Li Yazhao, Xie Jin, Su...</td>\n",
       "      <td>tju-dhd</td>\n",
       "      <td>https://doi.org/10.1109/TIP.2020.3034487</td>\n",
       "      <td>https://github.com/tjubiit/TJU-DHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-07-14</td>\n",
       "      <td>journal</td>\n",
       "      <td>Dual-resolution dual-path convolutional neural...</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>Downsampling input images is a simple trick to...</td>\n",
       "      <td>Pan Jing, Sun Hanqing, Song Zhanjie, Han Jungo...</td>\n",
       "      <td>dualnet</td>\n",
       "      <td>https://doi.org/10.3390/s19143111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>journal</td>\n",
       "      <td>GlanceNets - efficient convolutional neural ne...</td>\n",
       "      <td>Science China Information Sciences</td>\n",
       "      <td>Despite the success of CNNs, it is impeded to ...</td>\n",
       "      <td>Sun Hanqing, Pang Yanwei. GlanceNets - efficie...</td>\n",
       "      <td>glancenet</td>\n",
       "      <td>https://doi.org/10.1007/s11432-018-9497-0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pub_date category                                              title  \\\n",
       "0  2025-02-13  journal  Motion Expressions Guided Video Segmentation V...   \n",
       "1  2024-12-25  journal  Video instance segmentation without using mask...   \n",
       "2  2024-09-30  journal  Transformer-based stereo-aware 3D object detec...   \n",
       "3  2024-04-17  journal  Remote sensing image dehazing via a local cont...   \n",
       "4  2023-10-14  journal  Deep intra-image contrastive learning for weak...   \n",
       "5  2023-06-20  journal  Supervised biadjacency networks for stereo mat...   \n",
       "6  2023-02-05  journal  Semantic-aware self-supervised depth estimatio...   \n",
       "7  2020-11-03  journal  TJU-DHD: A diverse high-resolution dataset for...   \n",
       "8  2019-07-14  journal  Dual-resolution dual-path convolutional neural...   \n",
       "9  2018-09-03  journal  GlanceNets - efficient convolutional neural ne...   \n",
       "\n",
       "                                               venue  \\\n",
       "0  IEEE Transactions on Emerging Topics in Comput...   \n",
       "1                    IEEE Transactions on Multimedia   \n",
       "2  IEEE Transactions on Intelligent Transportatio...   \n",
       "3                                     Remote Sensing   \n",
       "4                                Pattern Recognition   \n",
       "5                  Multimedia Tools and Applications   \n",
       "6                        Pattern Recognition Letters   \n",
       "7              IEEE Transactions on Image Processing   \n",
       "8                                            Sensors   \n",
       "9                 Science China Information Sciences   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  Motion expressions guided video segmentation i...   \n",
       "1  Video instance segmentation (VIS) is a challen...   \n",
       "2  Transformers have shown promising progress in ...   \n",
       "3  Remote sensing image dehazing is a well-known ...   \n",
       "4  Weakly supervised person search aims to perfor...   \n",
       "5  Convolutional neural network (CNN) based stere...   \n",
       "6  Besides the 3D object supervision, the auxilia...   \n",
       "7  Vehicles, pedestrians, and riders are the most...   \n",
       "8  Downsampling input images is a simple trick to...   \n",
       "9  Despite the success of CNNs, it is impeded to ...   \n",
       "\n",
       "                                            citation      url_slug  \\\n",
       "0  Li Ge, Sun Hanqing, Yang Aiping, Cao Jiale, an...          emim   \n",
       "1  Li Ge, Cao Jiale, Sun Hanqing, Anwer Rao M., X...        mifvis   \n",
       "2  Sun Hanqing, Pang Yanwei, Cao Jiale, Xie Jin, ...          ts3d   \n",
       "3  Nie Jing, Xie Jin, Sun Hanqing. Remote sensing...     lceformer   \n",
       "4  Wang Jiabei, Pang Yanwei, Cao Jiale, Sun Hanqi...          dicl   \n",
       "5  Sun Hanqing, Han Jungong, Pang Yanwei, Li Xuel...  superbstereo   \n",
       "6  Sun Hanqing, Cao Jiale, Pang Yanwei. Semantic-...           s3d   \n",
       "7  Pang Yanwei, Cao Jiale, Li Yazhao, Xie Jin, Su...       tju-dhd   \n",
       "8  Pan Jing, Sun Hanqing, Song Zhanjie, Han Jungo...       dualnet   \n",
       "9  Sun Hanqing, Pang Yanwei. GlanceNets - efficie...     glancenet   \n",
       "\n",
       "                                      paper_url  \\\n",
       "0    https://doi.org/10.1109/TETCI.2025.3537936   \n",
       "1      https://doi.org/10.1109/TMM.2024.3521668   \n",
       "2     https://doi.org/10.1109/TITS.2024.3462795   \n",
       "3            https://doi.org/10.3390/rs16081422   \n",
       "4  https://doi.org/10.1016/j.patcog.2023.110047   \n",
       "5    https://doi.org/10.1007/s11042-023-15362-5   \n",
       "6  https://doi.org/10.1016/j.patrec.2023.02.006   \n",
       "7      https://doi.org/10.1109/TIP.2020.3034487   \n",
       "8             https://doi.org/10.3390/s19143111   \n",
       "9     https://doi.org/10.1007/s11432-018-9497-0   \n",
       "\n",
       "                          project_url  \n",
       "0                                 NaN  \n",
       "1                                 NaN  \n",
       "2                                 NaN  \n",
       "3                                 NaN  \n",
       "4                                 NaN  \n",
       "5                                 NaN  \n",
       "6                                 NaN  \n",
       "7  https://github.com/tjubiit/TJU-DHD  \n",
       "8                                 NaN  \n",
       "9                                 NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\n",
    "publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for row, item in publications.iterrows():\n",
    "\n",
    "    md_filename = str(item.pub_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.pub_date) + \"-\" + item.url_slug\n",
    "    year = item.pub_date[:4]\n",
    "\n",
    "    ## YAML variables\n",
    "\n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "\n",
    "    md += \"\"\"collection: publications\\n\"\"\"\n",
    "\n",
    "    md += \"\"\"category: \"\"\" + item.category\n",
    "\n",
    "    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n",
    "\n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\nexcerpt: '\" + html_escape(item.excerpt) + \"'\"\n",
    "\n",
    "    md += \"\\ndate: \" + str(item.pub_date)\n",
    "\n",
    "    md += \"\\nvenue: '\" + html_escape(item.venue) + \"'\"\n",
    "\n",
    "    if len(str(item.project_url)) > 5:\n",
    "        md += \"\\nprojecturl: '\" + item.project_url + \"'\"\n",
    "\n",
    "    if len(str(item.paper_url)) > 5:\n",
    "        md += \"\\npaperurl: '\" + item.paper_url + \"'\"\n",
    "\n",
    "    md += \"\\ncitation: '\" + html_escape(item.citation) + \"'\"\n",
    "\n",
    "    md += \"\\n---\"\n",
    "\n",
    "    ## Markdown description for individual page\n",
    "\n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n",
    "\n",
    "    # if len(str(item.project_url)) > 5:\n",
    "    #     md += \"\\n[Project page](\" + item.project_url + \")\\n\"\n",
    "\n",
    "    # if len(str(item.paper_url)) > 5:\n",
    "    #     md += \"\\n[Download paper here](\" + item.paper_url + \")\\n\"\n",
    "\n",
    "    # md += \"\\nRecommended citation: \" + item.citation\n",
    "\n",
    "    md_filename = os.path.basename(md_filename)\n",
    "\n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the publications directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-03-glancenet.md    2023-10-14-dicl.md\n",
      "2019-07-14-dualnet.md      2024-04-17-lceformer.md\n",
      "2020-11-03-tju-dhd.md      2024-09-30-ts3d.md\n",
      "2023-02-05-s3d.md          2024-12-25-mifvis.md\n",
      "2023-06-20-superbstereo.md 2025-02-13-emim.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_publications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Transformer-based stereo-aware 3D object detection from binocular images\"\n",
      "collection: publications\n",
      "category: journal\n",
      "permalink: /publication/2024-09-30-ts3d\n",
      "excerpt: 'Transformers have shown promising progress in various visual object detection tasks, including monocular 2D/3D detection and surround-view 3D detection. More importantly, the attention mechanism in the Transformer model and the 3D information extraction in binocular stereo are both similaritybased. However, directly applying existing Transformer-based detectors to binocular stereo 3D object detection leads to slow convergence and significant precision drops. We argue that a key cause of that defect is that existing Transformers ignore the binocular-stereo-specific image correspondence information. In this paper, we explore the model design of Transformers in binocular 3D object detection, focusing particularly on extracting and encoding task-specific image correspondence information. To achieve this goal, we present TS3D, a Transformer-based Stereo-aware 3D object detector. In the TS3D, a DisparityAware Positional Encoding (DAPE) module is proposed to embed the image correspondence information into stereo features. The correspondence is encoded as normalized sub-pixel-level disparity and is used in conjunction with sinusoidal 2D positional encoding to provide the 3D location information of the scene. To enrich multi-scale stereo features, we propose a Stereo Preserving Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the correspondence information while fusing intra-scale and aggregating cross-scale stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection average precision on the KITTI test set and takes 88 ms to detect objects from each binocular image pair. It is competitive with advanced counterparts in terms of both precision and inference speed.'\n",
      "date: 2024-09-30\n",
      "venue: 'IEEE Transactions on Intelligent Transportation Systems'\n",
      "paperurl: 'https://doi.org/10.1109/TITS.2024.3462795'\n",
      "citation: 'Sun Hanqing, Pang Yanwei, Cao Jiale, Xie Jin, Li Xuelong. Transformer-based stereo-aware 3D object detection from binocular images. <i>IEEE Transactions on Intelligent Transportation Systems</i>, 2024, 25(12): 19675-19687.'\n",
      "---\n",
      "Transformers have shown promising progress in various visual object detection tasks, including monocular 2D/3D detection and surround-view 3D detection. More importantly, the attention mechanism in the Transformer model and the 3D information extraction in binocular stereo are both similaritybased. However, directly applying existing Transformer-based detectors to binocular stereo 3D object detection leads to slow convergence and significant precision drops. We argue that a key cause of that defect is that existing Transformers ignore the binocular-stereo-specific image correspondence information. In this paper, we explore the model design of Transformers in binocular 3D object detection, focusing particularly on extracting and encoding task-specific image correspondence information. To achieve this goal, we present TS3D, a Transformer-based Stereo-aware 3D object detector. In the TS3D, a DisparityAware Positional Encoding (DAPE) module is proposed to embed the image correspondence information into stereo features. The correspondence is encoded as normalized sub-pixel-level disparity and is used in conjunction with sinusoidal 2D positional encoding to provide the 3D location information of the scene. To enrich multi-scale stereo features, we propose a Stereo Preserving Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the correspondence information while fusing intra-scale and aggregating cross-scale stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection average precision on the KITTI test set and takes 88 ms to detect objects from each binocular image pair. It is competitive with advanced counterparts in terms of both precision and inference speed.\n"
     ]
    }
   ],
   "source": [
    "!cat ../_publications/2024-09-30-ts3d.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
